# ===============================================================================
# SRI LANKAN TEA PRICE PREDICTION HUB - FINAL WORKING VERSION
# Optimized and debugged ML pipeline with enhanced stability
# ===============================================================================

# STEP 1: Install and Import Required Libraries
!pip install -q gspread pandas gspread-dataframe xgboost lightgbm catboost scikit-learn optuna plotly seaborn

import pandas as pd
import numpy as np
import gspread
from google.colab import auth
from google.auth import default
from gspread_dataframe import set_with_dataframe
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.metrics import mean_absolute_error
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor
from datetime import datetime
import warnings
import optuna
import logging
import traceback
from typing import Dict, List, Tuple, Optional, Any
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Configure logging and warnings
warnings.filterwarnings('ignore')
optuna.logging.set_verbosity(optuna.logging.WARNING)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

# ===============================================================================
# OPTIMIZED CONFIGURATION CLASS
# ===============================================================================
class TeaMarketConfig:
    """Optimized configuration class for Sri Lankan Tea Market predictions"""
    
    # Training Configuration
    ENABLE_DEEP_TRAINING = True
    OPTUNA_TRIALS = 30  # Reduced for faster optimization
    CROSS_VALIDATION_FOLDS = 3
    ENSEMBLE_VOTING = True
    
    # Google Sheets Configuration
    SOURCE_SHEET_NAME = "Elevation Avg"
    SOURCE_TAB_NAME = "Data"
    PREDICTION_HUB_SHEET_NAME = "Tea_Price_Prediction_Hub_Final"
    
    # Column Mapping
    COLUMN_MAP = {
        "year": ["Year", "YEAR", "year"],
        "sale_no": ["Sale No", "SALE_NO", "sale_no", "Week"],
        "elevation": ["Elevation", "ELEVATION", "elevation"],
        "quantity": ["Quantity", "QUANTITY", "quantity"],
        "price": ["Average Price", "Price", "PRICE", "price"]
    }
    
    # Feature Engineering Parameters
    MAX_SALES_PER_YEAR = 52
    MIN_RECORDS_PER_ELEVATION = 20
    LAG_PERIODS = [1, 2, 4, 8, 12]  # Reduced lag periods
    ROLLING_WINDOWS = [4, 8, 12]  # Reduced rolling windows
    
    # Model Parameters
    MODEL_TYPES = ['lightgbm', 'xgboost', 'catboost']

config = TeaMarketConfig()

# ===============================================================================
# OPTIMIZED DATA LOADER
# ===============================================================================
class DataLoader:
    """Optimized data loading with error handling"""
    
    @staticmethod
    def authenticate_and_connect() -> gspread.Client:
        """Authenticate with Google Sheets API"""
        try:
            auth.authenticate_user()
            creds, _ = default()
            return gspread.authorize(creds)
        except Exception as e:
            logger.error(f"Authentication failed: {str(e)}")
            raise

    @staticmethod
    def load_data(gc: gspread.Client) -> Tuple[pd.DataFrame, str]:
        """Load and validate data from Google Sheets"""
        try:
            source_spreadsheet = gc.open(config.SOURCE_SHEET_NAME)
            source_worksheet = source_spreadsheet.worksheet(config.SOURCE_TAB_NAME)
            raw_data = source_worksheet.get_all_records()
            df = pd.DataFrame(raw_data)
            
            if df.empty:
                raise ValueError("No data found in the spreadsheet")
            
            # Clean column names
            df.columns = df.columns.str.strip()
            
            # Map columns
            column_mapping = {}
            for standard_col, possible_names in config.COLUMN_MAP.items():
                for possible_name in possible_names:
                    if possible_name in df.columns:
                        column_mapping[possible_name] = standard_col
                        break
            
            df.rename(columns=column_mapping, inplace=True)
            
            # Validate required columns
            required_cols = list(config.COLUMN_MAP.keys())
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                raise ValueError(f"Missing required columns: {missing_cols}")
            
            return df, source_spreadsheet.url
            
        except Exception as e:
            logger.error(f"Error loading data: {str(e)}")
            raise

# ===============================================================================
# STREAMLINED DATA PREPROCESSOR
# ===============================================================================
class DataPreprocessor:
    """Streamlined data preprocessing"""
    
    @staticmethod
    def clean_data(df: pd.DataFrame) -> pd.DataFrame:
        """Clean and validate data"""
        try:
            # Clean elevation data
            if 'elevation' in df.columns:
                df['elevation'] = df['elevation'].astype(str).str.strip().str.upper()
            
            # Convert numeric columns
            numeric_cols = ['year', 'sale_no', 'quantity', 'price']
            for col in numeric_cols:
                if col in df.columns:
                    if df[col].dtype == 'object':
                        df[col] = df[col].astype(str).str.replace(r'[^\d.]', '', regex=True)
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # Remove invalid rows
            df = df.dropna(subset=['year', 'sale_no', 'elevation', 'quantity', 'price'])
            df = df[(df['quantity'] > 0) & (df['price'] > 0)]
            df = df[(df['sale_no'] >= 1) & (df['sale_no'] <= config.MAX_SALES_PER_YEAR)]
            
            # Remove price outliers
            def remove_outliers(group):
                Q1 = group['price'].quantile(0.25)
                Q3 = group['price'].quantile(0.75)
                IQR = Q3 - Q1
                lower = Q1 - 1.5 * IQR
                upper = Q3 + 1.5 * IQR
                return group[(group['price'] >= lower) & (group['price'] <= upper)]
            
            df = df.groupby('elevation', group_keys=False).apply(remove_outliers)
            
            # Convert data types
            df = df.astype({
                'year': int,
                'sale_no': int,
                'elevation': str,
                'quantity': float,
                'price': float
            })
            
            return df.sort_values(['elevation', 'year', 'sale_no']).reset_index(drop=True)
            
        except Exception as e:
            logger.error(f"Data cleaning failed: {str(e)}")
            raise

# ===============================================================================
# OPTIMIZED FEATURE ENGINEER
# ===============================================================================
class FeatureEngineer:
    """Optimized feature engineering"""
    
    @staticmethod
    def create_features(df: pd.DataFrame) -> pd.DataFrame:
        """Create time-series features"""
        try:
            def engineer_features(group):
                group = group.copy().sort_values(['year', 'sale_no'])
                
                # Lag features
                for lag in config.LAG_PERIODS:
                    group[f'price_lag_{lag}'] = group['price'].shift(lag)
                
                # Rolling features
                for window in config.ROLLING_WINDOWS:
                    group[f'price_ma_{window}'] = group['price'].rolling(window=window, min_periods=1).mean()
                    group[f'price_std_{window}'] = group['price'].rolling(window=window, min_periods=1).std()
                
                # Price change features
                group['price_change_1w'] = group['price'] - group['price'].shift(1)
                group['price_pct_change_4w'] = group['price'].pct_change(4)
                
                # Time index
                group['time_idx'] = group['year'] * config.MAX_SALES_PER_YEAR + group['sale_no']
                
                return group
            
            df_featured = df.groupby('elevation', group_keys=False).apply(engineer_features)
            return df_featured.fillna(method='ffill').fillna(method='bfill').fillna(0)
            
        except Exception as e:
            logger.error(f"Feature engineering failed: {str(e)}")
            raise

# ===============================================================================
# OPTIMIZED ENSEMBLE PREDICTOR
# ===============================================================================
class EnsemblePredictor:
    """Optimized ensemble predictor"""
    
    def __init__(self, feature_cols: List[str], elevation: str):
        self.feature_cols = feature_cols
        self.elevation = elevation
        self.scaler = RobustScaler()
        self.models = {}
        self.model_weights = {}
        
    def train(self, df_elevation: pd.DataFrame, deep_train: bool = True):
        """Train ensemble models"""
        try:
            X = df_elevation[self.feature_cols]
            y = np.log1p(df_elevation['price'])  # Log transform
            X_scaled = self.scaler.fit_transform(X)
            
            if deep_train:
                self._train_with_optuna(X_scaled, y)
            else:
                self._train_default(X_scaled, y)
                
        except Exception as e:
            logger.error(f"Training failed for {self.elevation}: {str(e)}")
            raise
    
    def _train_with_optuna(self, X, y):
        """Train with Optuna optimization"""
        cv_scores = {}
        
        # LightGBM
        study = optuna.create_study(direction='minimize')
        study.optimize(lambda trial: self._lgbm_objective(trial, X, y), 
                      n_trials=config.OPTUNA_TRIALS//3)
        best_lgbm = LGBMRegressor(**study.best_params)
        best_lgbm.fit(X, y)
        self.models['lightgbm'] = best_lgbm
        cv_scores['lightgbm'] = study.best_value
        
        # XGBoost
        study = optuna.create_study(direction='minimize')
        study.optimize(lambda trial: self._xgb_objective(trial, X, y), 
                      n_trials=config.OPTUNA_TRIALS//3)
        best_xgb = XGBRegressor(**study.best_params)
        best_xgb.fit(X, y)
        self.models['xgboost'] = best_xgb
        cv_scores['xgboost'] = study.best_value
        
        # CatBoost
        study = optuna.create_study(direction='minimize')
        study.optimize(lambda trial: self._catboost_objective(trial, X, y), 
                      n_trials=config.OPTUNA_TRIALS//3)
        best_cat = CatBoostRegressor(**study.best_params, verbose=False)
        best_cat.fit(X, y)
        self.models['catboost'] = best_cat
        cv_scores['catboost'] = study.best_value
        
        # Calculate weights
        total = sum(1/(score + 1e-8) for score in cv_scores.values())
        self.model_weights = {model: (1/(score + 1e-8))/total for model, score in cv_scores.items()}
    
    def _train_default(self, X, y):
        """Train with default parameters"""
        self.models = {
            'lightgbm': LGBMRegressor(),
            'xgboost': XGBRegressor(),
            'catboost': CatBoostRegressor(verbose=False)
        }
        
        for name, model in self.models.items():
            model.fit(X, y)
        
        # Equal weights
        self.model_weights = {name: 1/len(self.models) for name in self.models.keys()}
    
    def predict(self, df_elevation: pd.DataFrame) -> Dict[str, float]:
        """Generate predictions"""
        try:
            X_latest = df_elevation[self.feature_cols].iloc[-1:]
            X_latest_scaled = self.scaler.transform(X_latest)
            
            predictions = {}
            for name, model in self.models.items():
                pred_log = model.predict(X_latest_scaled)[0]
                predictions[name] = np.expm1(pred_log)
            
            # Weighted average
            ensemble_pred = sum(pred * self.model_weights[name] 
                              for name, pred in predictions.items())
            
            # Confidence interval
            pred_std = np.std(list(predictions.values()))
            return {
                'prediction': ensemble_pred,
                'confidence_lower': max(0, ensemble_pred - 1.96 * pred_std),
                'confidence_upper': ensemble_pred + 1.96 * pred_std,
                'uncertainty': pred_std
            }
            
        except Exception as e:
            logger.error(f"Prediction failed for {self.elevation}: {str(e)}")
            raise
    
    # Objective functions for Optuna
    def _lgbm_objective(self, trial, X, y):
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 100, 500),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
            'num_leaves': trial.suggest_int('num_leaves', 10, 50),
            'max_depth': trial.suggest_int('max_depth', 3, 10)
        }
        model = LGBMRegressor(**params)
        score = -cross_val_score(model, X, y, cv=TimeSeriesSplit(config.CROSS_VALIDATION_FOLDS),
                               scoring='neg_mean_absolute_error').mean()
        return score
    
    def _xgb_objective(self, trial, X, y):
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 100, 500),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0)
        }
        model = XGBRegressor(**params)
        score = -cross_val_score(model, X, y, cv=TimeSeriesSplit(config.CROSS_VALIDATION_FOLDS),
                               scoring='neg_mean_absolute_error').mean()
        return score
    
    def _catboost_objective(self, trial, X, y):
        params = {
            'iterations': trial.suggest_int('iterations', 100, 500),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
            'depth': trial.suggest_int('depth', 3, 10)
        }
        model = CatBoostRegressor(**params, verbose=False)
        score = -cross_val_score(model, X, y, cv=TimeSeriesSplit(config.CROSS_VALIDATION_FOLDS),
                               scoring='neg_mean_absolute_error').mean()
        return score

# ===============================================================================
# SIMPLIFIED REPORT GENERATOR
# ===============================================================================
class ReportGenerator:
    """Simplified report generation"""
    
    @staticmethod
    def update_prediction_hub(gc: gspread.Client, predictions: List[Dict]):
        """Update Google Sheets with predictions"""
        try:
            # Create or open spreadsheet
            try:
                sh = gc.open(config.PREDICTION_HUB_SHEET_NAME)
            except gspread.exceptions.SpreadsheetNotFound:
                sh = gc.create(config.PREDICTION_HUB_SHEET_NAME)
            
            # Update predictions sheet
            try:
                ws = sh.worksheet("Predictions")
            except gspread.exceptions.WorksheetNotFound:
                ws = sh.add_worksheet(title="Predictions", rows=1000, cols=20)
            
            # Prepare data
            df = pd.DataFrame(predictions)
            df['Timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            # Update sheet
            if not ws.row_values(1):  # Empty sheet
                ws.append_row(df.columns.tolist())
            ws.append_rows(df.values.tolist())
            
            return sh.url
            
        except Exception as e:
            logger.error(f"Failed to update prediction hub: {str(e)}")
            raise

# ===============================================================================
# OPTIMIZED MAIN PIPELINE
# ===============================================================================
class TeaPricePredictor:
    """Optimized main prediction pipeline"""
    
    def __init__(self):
        self.data_loader = DataLoader()
        self.preprocessor = DataPreprocessor()
        self.feature_engineer = FeatureEngineer()
        self.report_generator = ReportGenerator()
    
    def run(self):
        """Execute the prediction pipeline"""
        logger.info("Starting Tea Price Prediction Pipeline")
        
        try:
            # Step 1: Load data
            gc = self.data_loader.authenticate_and_connect()
            df, _ = self.data_loader.load_data(gc)
            
            # Step 2: Clean data
            df_clean = self.preprocessor.clean_data(df)
            if df_clean.empty:
                raise ValueError("No valid data remaining after cleaning")
            
            # Step 3: Feature engineering
            df_featured = self.feature_engineer.create_features(df_clean)
            feature_cols = [col for col in df_featured.columns 
                          if col not in ['elevation', 'year', 'sale_no', 'quantity', 'price']]
            
            # Step 4: Train models and predict
            predictions = []
            for elevation in df_featured['elevation'].unique():
                try:
                    elevation_data = df_featured[df_featured['elevation'] == elevation]
                    if len(elevation_data) < config.MIN_RECORDS_PER_ELEVATION:
                        continue
                    
                    # Initialize and train predictor
                    predictor = EnsemblePredictor(feature_cols, elevation)
                    predictor.train(elevation_data, config.ENABLE_DEEP_TRAINING)
                    
                    # Generate prediction
                    pred_result = predictor.predict(elevation_data)
                    
                    # Determine next sale period
                    latest = elevation_data.iloc[-1]
                    next_sale = latest['sale_no'] + 1
                    next_year = latest['year']
                    if next_sale > config.MAX_SALES_PER_YEAR:
                        next_sale = 1
                        next_year += 1
                    
                    # Store results
                    predictions.append({
                        'Elevation': elevation,
                        'Predicted_Year': int(next_year),
                        'Predicted_Sale_No': int(next_sale),
                        'Predicted_Price': round(pred_result['prediction'], 2),
                        'Confidence_Lower': round(pred_result['confidence_lower'], 2),
                        'Confidence_Upper': round(pred_result['confidence_upper'], 2),
                        'Uncertainty': round(pred_result['uncertainty'], 2),
                        'Data_Points': len(elevation_data)
                    })
                    
                    logger.info(f"Processed {elevation}: Rs. {pred_result['prediction']:.2f}")
                    
                except Exception as e:
                    logger.error(f"Error processing {elevation}: {str(e)}")
                    continue
            
            # Step 5: Save results
            if predictions:
                hub_url = self.report_generator.update_prediction_hub(gc, predictions)
                logger.info(f"Predictions saved to: {hub_url}")
                return predictions
            else:
                raise ValueError("No predictions generated")
                
        except Exception as e:
            logger.error(f"Pipeline failed: {str(e)}")
            raise

# ===============================================================================
# EXECUTION
# ===============================================================================
if __name__ == "__main__":
    try:
        predictor = TeaPricePredictor()
        results = predictor.run()
        print("\nPrediction completed successfully!")
        print(pd.DataFrame(results))
    except Exception as e:
        print(f"\nPrediction failed: {str(e)}")